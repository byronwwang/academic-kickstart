@article{Komkov2019,
abstract = {In this paper we propose a novel easily reproducible technique to attack the best public Face ID system ArcFace in different shooting conditions. To create an attack, we print the rectangular paper sticker on a common color printer and put it on the hat. The adversarial sticker is prepared with a novel algorithm for off-plane transformations of the image which imitates sticker location on the hat. Such an approach confuses the state-of-the-art public Face ID model LResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID models.},
archivePrefix = {arXiv},
arxivId = {1908.08705},
author = {Komkov, Stepan and Petiushko, Aleksandr},
eprint = {1908.08705},
journal = {arXiv preprint arXiv:1908.08705},
title = {{AdvHat: Real-world adversarial attack on ArcFace Face ID system}},
url = {http://arxiv.org/abs/1908.08705},
year = {2019}
}
@inproceedings{Wang2018a,
abstract = {Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: Maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.},
archivePrefix = {arXiv},
arxivId = {1801.09414},
author = {Wang, Hao and Wang, Yitong and Zhou, Zheng and Ji, Xing and Gong, Dihong and Zhou, Jingchao and Li, Zhifeng and Liu, Wei},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00552},
eprint = {1801.09414},
isbn = {978-1-5386-6420-9},
issn = {10636919},
month = {jun},
pages = {5265--5274},
publisher = {IEEE},
title = {{CosFace: Large Margin Cosine Loss for Deep Face Recognition}},
url = {https://ieeexplore.ieee.org/document/8578650/},
year = {2018}
}
@inproceedings{shi2019curls,
author = {Shi, Yucheng and Wang, Siyu and Han, Yahong},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
pages = {6519--6527},
title = {{Curls & Whey: Boosting Black-Box Adversarial Attacks}},
year = {2019}
}
@inproceedings{Dong2019,
abstract = {Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efficiency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-defined kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.},
archivePrefix = {arXiv},
arxivId = {1904.02884},
author = {Dong, Yinpeng and Pang, Tianyu and Su, Hang and Zhu, Jun},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
eprint = {1904.02884},
month = {apr},
pages = {4312----4321},
title = {{Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks}},
url = {http://arxiv.org/abs/1904.02884},
year = {2019}
}
@article{RN48,
abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.1},
archivePrefix = {arXiv},
arxivId = {1511.04599},
author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
doi = {10.1109/CVPR.2016.282},
eprint = {1511.04599},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {jun},
pages = {2574--2582},
publisher = {IEEE},
title = {{DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks}},
url = {http://ieeexplore.ieee.org/document/7780651/},
year = {2016}
}
@article{Biggio2013,
abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis. {\textcopyright} 2013 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {1708.06131},
author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'{c}}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
doi = {10.1007/978-3-642-40994-3_25},
eprint = {1708.06131},
isbn = {9783642409936},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {adversarial machine learning,evasion attacks,neural networks,support vector machines},
number = {PART 3},
pages = {387--402},
title = {{Evasion attacks against machine learning at test time}},
volume = {8190 LNAI},
year = {2013}
}
@inproceedings{Athalye2018a,
abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that arc adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.},
archivePrefix = {arXiv},
arxivId = {1707.07397},
author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kevin, Kwok},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1707.07397},
isbn = {9781510867963},
pages = {449--468},
title = {{Synthesizing robust adversarial examples}},
volume = {1},
year = {2018}
}
@inproceedings{Xie2019a,
abstract = {Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018--it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by $\sim$10%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training.},
archivePrefix = {arXiv},
arxivId = {1812.03411},
author = {Xie, Cihang and Wu, Yuxin and Maaten, Laurens Van Der and Yuille, Alan L. and He, Kaiming},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2019.00059},
eprint = {1812.03411},
isbn = {978-1-7281-3293-8},
issn = {10636919},
keywords = {Categorization,Deep Learning,Recognition: Detection,Retrieval},
month = {jun},
pages = {501--509},
publisher = {IEEE},
title = {{Feature Denoising for Improving Adversarial Robustness}},
url = {https://ieeexplore.ieee.org/document/8954372/},
volume = {2019-June},
year = {2019}
}
@article{Baluja2018,
abstract = {With the rapidly increasing popularity of deep neural networks for image recognition tasks, a parallel interest in generating adversarial examples to attack the trained models has arisen. To date, these approaches have involved either directly computing gradients with respect to the image pixels or directly solving an optimization on the image pixels. We generalize this pursuit in a novel direction: can a separate network be trained to efficiently attack another fully trained network? We demonstrate that it is possible, and that the generated attacks yield startling insights into the weaknesses of the target network. We call such a network an Adversarial Transformation Network (ATN). ATNs transform any input into an adversarial attack on the target network, while being minimally perturbing to the original inputs and the target network's outputs. Further, we show that ATNs are capable of not only causing the target network to make an error, but can be constructed to explicitly control the type of misclassification made. We demonstrate ATNs on both simple MNIST-digit classifiers and state-of-the-art ImageNet classifiers deployed by Google, Inc.: Inception ResNet-v2.},
author = {Baluja, Shumeet and Fischer, Ian},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
number = {1},
pages = {2687--2695},
title = {{Learning to attack: Adversarial transformation networks}},
year = {2018}
}
@inproceedings{Redmon2017,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.690},
eprint = {1612.08242},
isbn = {978-1-5386-0457-1},
month = {jul},
pages = {6517--6525},
publisher = {IEEE},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://ieeexplore.ieee.org/document/8100173/},
year = {2017}
}
@article{Xiao2019,
author = {Xiao, Kai Y and Tjeng, Vincent and Muhammad, Nur and Shafiullah, Mahi},
pages = {1--20},
title = {{T RAINING FOR F ASTER A DVERSARIAL R OBUSTNESS V ERIFICATION VIA I NDUCING R E LU S TABILITY}},
year = {2019}
}
@article{Suya2019,
abstract = {We study adversarial examples in a black-box setting where the adversary only has API access to the target model and each query is expensive. Prior work on black-box adversarial examples follows one of two main strategies: (1) transfer attacks use white-box attacks on local models to find candidate adversarial examples that transfer to the target model, and (2) optimization-based attacks use queries to the target model and apply optimization techniques to search for adversarial examples. We propose hybrid attacks that combine both strategies, using candidate adversarial examples from local models as starting points for optimization-based attacks and using labels learned in optimization-based attacks to tune local models for finding transfer candidates. We empirically demonstrate on the MNIST, CIFAR10, and ImageNet datasets that our hybrid attack strategy reduces cost and improves success rates. We also introduce a seed prioritization strategy which enables attackers to focus their resources on the most promising seeds. Combining hybrid attacks with our seed prioritization strategy enables batch attacks that can reliably find adversarial examples with only a handful of queries.},
archivePrefix = {arXiv},
arxivId = {1908.07000},
author = {Suya, Fnu and Chi, Jianfeng and Evans, David and Tian, Yuan},
eprint = {1908.07000},
number = {August},
title = {{Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries}},
url = {http://arxiv.org/abs/1908.07000},
volume = {2020},
year = {2019}
}
@inproceedings{pmlr-v80-ilyas18a,
abstract = {Current neural network-based classifiers are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model. In practice, the threat model for real-world systems is often more restrictive than the typical black-box model where the adversary can observe the full output of the network on arbitrarily many chosen inputs. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the partialinformation setting, and the label-only setting. We develop new attacks that fool classifiers under these more restrictive threat models, where previous methods would be impractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our proposed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, partial information, and other practical issues to break the Google Cloud Vision API.},
address = {Stockholmsm{\"{a}}ssan, Stockholm Sweden},
archivePrefix = {arXiv},
arxivId = {1804.08598},
author = {Eyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
editor = {Dy, Jennifer and Krause, Andreas},
eprint = {1804.08598},
isbn = {9781510867963},
pages = {3392--3401},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Black-box adversarial attacks with limited queries and information}},
url = {http://proceedings.mlr.press/v80/ilyas18a.html},
volume = {5},
year = {2018}
}
@inproceedings{Jang2017a,
abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
address = {New York, New York, USA},
author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
booktitle = {Proceedings of the 33rd Annual Computer Security Applications Conference on - ACSAC 2017},
doi = {10.1145/3134600.3134635},
isbn = {9781450353458},
keywords = {Adversarial Examples,Machine Learning},
pages = {262--277},
publisher = {ACM Press},
title = {{Objective Metrics and Gradient Descent Algorithms for Adversarial Examples in Machine Learning}},
url = {http://dl.acm.org/citation.cfm?doid=3134600.3134635},
volume = {Part F1325},
year = {2017}
}
@article{Brendel2020,
abstract = {The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate measurable progress towards robust machine vision models and more generally applicable adversarial attacks. This document is an updated version of our competition proposal that was accepted in the competition track of 32nd Conference on Neural Information Processing Systems (NIPS 2018).},
archivePrefix = {arXiv},
arxivId = {1808.01976},
author = {Brendel, Wieland and Rauber, Jonas and Kurakin, Alexey and Papernot, Nicolas and Veliqi, Behar and Mohanty, Sharada P. and Laurent, Florian and Salath{\'{e}}, Marcel and Bethge, Matthias and Yu, Yaodong and Zhang, Hongyang and Xu, Susu and Zhang, Hongbao and Xie, Pengtao and Xing, Eric P. and Brunner, Thomas and Diehl, Frederik and Rony, J{\'{e}}r{\^{o}}me and Hafemann, Luiz Gustavo and Cheng, Shuyu and Dong, Yinpeng and Ning, Xuefei and Li, Wenshuo and Wang, Yu},
doi = {10.1007/978-3-030-29135-8_5},
eprint = {1808.01976},
number = {Nips},
pages = {129--153},
title = {{Adversarial Vision Challenge}},
year = {2020}
}
@article{Li2018,
abstract = {Recent development of adversarial attacks has proven that ensemble-based methods outperform traditional, non-ensemble ones in black-box attack. However, as it is computationally prohibitive to acquire a family of diverse models, these methods achieve inferior performance constrained by the limited number of models to be ensembled. In this paper, we propose Ghost Networks to improve the transferability of adversarial examples. The critical principle of ghost networks is to apply feature-level perturbations to an existing model to potentially create a huge set of diverse models. After that, models are subsequently fused by longitudinal ensemble. Extensive experimental results suggest that the number of networks is essential for improving the transferability of adversarial examples, but it is less necessary to independently train different networks and ensemble them in an intensive aggregation way. Instead, our work can be used as a computationally cheap and easily applied plug-in to improve adversarial approaches both in single-model and multi-model attack, compatible with residual and non-residual networks. By reproducing the NeurIPS 2017 adversarial competition, our method outperforms the No.1 attack submission by a large margin, demonstrating its effectiveness and efficiency. Code is available at https://github.com/LiYingwei/ghost-network.},
archivePrefix = {arXiv},
arxivId = {1812.03413},
author = {Li, Yingwei and Bai, Song and Zhou, Yuyin and Xie, Cihang and Zhang, Zhishuai and Yuille, Alan},
eprint = {1812.03413},
journal = {arXiv preprint arXiv:1812.03413},
month = {dec},
title = {{Learning Transferable Adversarial Examples via Ghost Networks}},
url = {http://arxiv.org/abs/1812.03413},
year = {2018}
}
@article{Wang2004a,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
doi = {10.1109/TIP.2003.819861},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
month = {apr},
number = {4},
pages = {600--612},
pmid = {15376593},
title = {{Image Quality Assessment: From Error Visibility to Structural Similarity}},
url = {http://ieeexplore.ieee.org/document/1284395/},
volume = {13},
year = {2004}
}
@article{Liu2018,
abstract = {Deep Neural Networks (DNNs) have recently led to significant improvements in many fields. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Moreover, adversarial examples can be used to perform an attack on various kinds of DNN based systems, even if the adversary has no access to the underlying model. Many defense methods have been proposed, such as obfuscating gradients of the networks or detecting adversarial examples. However it is proved out that these defense methods are not effective or cannot resist secondary adversarial attacks. In this paper, we point out that steganalysis can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modifications caused by adversarial attacks. Experimental results show that the proposed method can accurately detect adversarial examples. Moreover, secondary adversarial attacks cannot be directly performed to our method because our method is not based on a neural network but based on high-dimensional artificial features and FLD (Fisher Linear Discriminant) ensemble.},
archivePrefix = {arXiv},
arxivId = {1806.09186},
author = {Liu, Jiayang and Zhang, Weiming and Zhang, Yiwei and Hou, Dongdong and Liu, Yujia and Zha, Hongyue and Yu, Nenghai},
eprint = {1806.09186},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {4825--4834},
title = {{Detection based Defense against Adversarial Examples from the Steganalysis Point of View}},
url = {http://arxiv.org/abs/1806.09186},
year = {2018}
}
@article{RN45,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examplesâ€”inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
month = {dec},
pages = {1--11},
title = {{Explaining and harnessing adversarial examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2015}
}
@inproceedings{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1406.2661},
issn = {10495258},
month = {oct},
pages = {2672--2680},
title = {{Generative adversarial nets}},
year = {2014}
}
@article{RN47,
abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
archivePrefix = {arXiv},
arxivId = {1706.06083},
author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
eprint = {1706.06083},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {jun},
title = {{Towards deep learning models resistant to adversarial attacks}},
url = {http://arxiv.org/abs/1706.06083},
year = {2018}
}
@article{RN63,
abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox).},
archivePrefix = {arXiv},
arxivId = {1712.04248},
author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
eprint = {1712.04248},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
title = {{Decision-based adversarial attacks: Reliable attacks against black-box machine learning models}},
year = {2018}
}
@inproceedings{DBLP:conf/icml/WongSK19,
abstract = {A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by lv norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance in image space. In the image classification setting, such distances measure the cost of moving pixel mass, which can naturally represent "standard" image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for approximate projection onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers, and code for all experiments in the paper is available at https://gi thub.com/locuslab/projected-sinkhorn.},
archivePrefix = {arXiv},
arxivId = {1902.07906},
author = {Wong, Erie and Schmidt, Frank R. and {Zico Kolter}, J.},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
eprint = {1902.07906},
isbn = {9781510886988},
pages = {11812--11825},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Wasserstein adversarial examples via projected sinkhorn iterations}},
url = {http://arxiv.org/abs/1902.07906},
volume = {2019-June},
year = {2019}
}
@inproceedings{Xie2018,
abstract = {Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https://github.com/cihangxie/DI-2-FGSM.},
archivePrefix = {arXiv},
arxivId = {1803.06978},
author = {Xie, Cihang and Zhang, Zhishuai and Zhou, Yuyin and Bai, Song and Wang, Jianyu and Ren, Zhou and Yuille, Alan},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
eprint = {1803.06978},
pages = {2730--2739},
title = {{Improving Transferability of Adversarial Examples with Input Diversity}},
url = {http://arxiv.org/abs/1803.06978},
year = {2018}
}
@inproceedings{RN53,
abstract = {Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.},
archivePrefix = {arXiv},
arxivId = {1710.06081},
author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00957},
eprint = {1710.06081},
isbn = {9781538664209},
issn = {10636919},
month = {jun},
pages = {9185--9193},
publisher = {IEEE},
title = {{Boosting Adversarial Attacks with Momentum}},
url = {https://ieeexplore.ieee.org/document/8579055/},
year = {2018}
}
@article{Carlini2016,
abstract = {We show that defensive distillation is not secure: it is no more resistant to targeted misclassification attacks than unprotected neural networks.},
archivePrefix = {arXiv},
arxivId = {1607.04311},
author = {Carlini, Nicholas and Wagner, David},
eprint = {1607.04311},
pages = {1--3},
title = {{Defensive Distillation is Not Robust to Adversarial Examples}},
url = {http://arxiv.org/abs/1607.04311},
volume = {0},
year = {2016}
}
@article{Zhang2018,
abstract = {Privacy-preserving releasing of complex data (e.g., image, text, audio) represents a long-standing challenge for the data mining research community. Due to rich semantics of the data and lack of a priori knowledge about the analysis task, excessive sanitization is often necessary to ensure privacy, leading to significant loss of the data utility. In this paper, we present dp-GAN, a general private releasing framework for semantic-rich data. Instead of sanitizing and then releasing the data, the data curator publishes a deep generative model which is trained using the original data in a differentially private manner; with the generative model, the analyst is able to produce an unlimited amount of synthetic data for arbitrary analysis tasks. In contrast of alternative solutions, dp-GAN highlights a set of key features: (i) it provides theoretical privacy guarantee via enforcing the differential privacy principle; (ii) it retains desirable utility in the released model, enabling a variety of otherwise impossible analyses; and (iii) most importantly, it achieves practical training scalability and stability by employing multi-fold optimization strategies. Through extensive empirical evaluation on benchmark datasets and analyses, we validate the efficacy of dp-GAN.},
archivePrefix = {arXiv},
arxivId = {1801.01594},
author = {Zhang, Xinyang and Ji, Shouling and Wang, Ting},
eprint = {1801.01594},
title = {{Differentially Private Releasing via Deep Generative Model (Technical Report)}},
url = {http://arxiv.org/abs/1801.01594},
year = {2018}
}
@article{Huster2019,
abstract = {Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses.},
archivePrefix = {arXiv},
arxivId = {1807.09705},
author = {Huster, Todd and Chiang, Cho Yu Jason and Chadha, Ritu},
doi = {10.1007/978-3-030-13453-2_2},
eprint = {1807.09705},
isbn = {9783030134525},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Adversarial examples,Lipschitz constant},
pages = {16--29},
title = {{Limitations of the Lipschitz Constant as a Defense Against Adversarial Examples}},
volume = {11329 LNAI},
year = {2019}
}
@incollection{NIPS2019_9368,
abstract = {In this work, we consider one challenging training time attack by modifying training data with bounded perturbation, hoping to manipulate the behavior (both targeted or non-targeted) of any corresponding trained classifier during test time when facing clean samples. To achieve this, we proposed to use an auto-encoder-like network to generate the pertubation on the training data paired with one differentiable system acting as the imaginary victim classifier. The perturbation generator will learn to update its weights by watching the training procedure of the imaginary classifier in order to produce the most harmful and imperceivable noise which in turn will lead the lowest generalization power for the victim classifier. This can be formulated into a non-linear equality constrained optimization problem. Unlike GANs, solving such problem is computationally challenging, we then proposed a simple yet effective procedure to decouple the alternating updates for the two networks for stability. The method proposed in this paper can be easily extended to the label specific setting where the attacker can manipulate the predictions of the victim classifiers according to some predefined rules rather than only making wrong predictions. Experiments on various datasets including CIFAR-10 and a reduced version of ImageNet confirmed the effectiveness of the proposed method and empirical results showed that, such bounded perturbation have good transferability regardless of which classifier the victim is actually using on image data.},
archivePrefix = {arXiv},
arxivId = {1905.09027},
author = {Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d\textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
eprint = {1905.09027},
pages = {11971--11981},
publisher = {Curran Associates, Inc.},
title = {{Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder}},
url = {http://arxiv.org/abs/1905.09027},
year = {2019}
}
@article{Croce2019,
abstract = {The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as the methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields high quality results already with one restart, minimizes the size of the perturbation, so that the robust accuracy can be evaluated at all possible thresholds with a single run, and comes with almost no free parameters except number of iterations and restarts. It achieves better or similar robust test accuracy compared to state-of-the-art attacks which are partially specialized to one $l_p$-norm.},
archivePrefix = {arXiv},
arxivId = {1907.02044},
author = {Croce, Francesco and Hein, Matthias},
eprint = {1907.02044},
pages = {1--17},
title = {{Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack}},
url = {http://arxiv.org/abs/1907.02044},
year = {2019}
}
@article{Xie2019,
abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A (+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images ($\sim$3000X more than ImageNet) and $\sim$9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1911.09665},
author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.},
eprint = {1911.09665},
month = {nov},
title = {{Adversarial Examples Improve Image Recognition}},
url = {http://arxiv.org/abs/1911.09665},
year = {2019}
}
@article{Gafni2019,
abstract = {We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time.},
archivePrefix = {arXiv},
arxivId = {1911.08348},
author = {Gafni, Oran and Wolf, Lior and Taigman, Yaniv},
eprint = {1911.08348},
title = {{Live Face De-Identification in Video}},
url = {http://arxiv.org/abs/1911.08348},
year = {2019}
}
@article{RN62,
abstract = {Deep Neural Networks have achieved extraordinary results on image classification tasks, but have been shown to be vulnerable to attacks with carefully crafted perturbations of the input data. Although most attacks usually change values of many image's pixels, it has been shown that deep networks are also vulnerable to sparse alterations of the input. However, no computationally efficient method has been proposed to compute sparse perturbations. In this paper, we exploit the low mean curvature of the decision boundary, and propose SparseFool, a geometry inspired sparse attack that controls the sparsity of the perturbations. Extensive evaluations show that our approach computes sparse perturbations very fast, and scales efficiently to high dimensional data. We further analyze the transferability and the visual effects of the perturbations, and show the existence of shared semantic information across the images and the networks. Finally, we show that adversarial training can only slightly improve the robustness against sparse additive perturbations computed with SparseFool.},
archivePrefix = {arXiv},
arxivId = {1811.02248},
author = {Modas, Apostolos and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
eprint = {1811.02248},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
month = {nov},
pages = {9087----9096},
title = {{SparseFool: a few pixels make a big difference}},
url = {http://arxiv.org/abs/1811.02248},
year = {2018}
}
@inproceedings{Thys2019,
abstract = {Adversarial attacks on machine learning models have seen increasing interest in the past years. By making only subtle changes to the input of a convolutional neural network, the output of the network can be swayed to output a completely different result. The first attacks did this by changing pixel values of an input image slightly to fool a classifier to output the wrong class. Other approaches have tried to learn "patches" that can be applied to an object to fool detectors and classifiers. Some of these approaches have also shown that these attacks are feasible in the real-world, i.e. by modifying an object and filming it with a video camera. However, all of these approaches target classes that contain almost no intra-class variety (e.g. stop signs). The known structure of the object is then used to generate an adversarial patch on top of it. In this paper, we present an approach to generate adversarial patches to targets with lots of intra-class variety, namely persons. The goal is to generate a patch that is able successfully hide a person from a person detector. An attack that could for instance be used maliciously to circumvent surveillance systems, intruders can sneak around undetected by holding a small cardboard plate in front of their body aimed towards the surveillance camera. From our results we can see that our system is able significantly lower the accuracy of a person detector. Our approach also functions well in real-life scenarios where the patch is filmed by a camera. To the best of our knowledge we are the first to attempt this kind of attack on targets with a high level of intra-class variety like persons.},
archivePrefix = {arXiv},
arxivId = {1904.08653},
author = {Thys, Simen and {Van Ranst}, Wiebe and Goedem{\'{e}}, Toon},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
eprint = {1904.08653},
pages = {0},
title = {{Fooling automated surveillance cameras: adversarial patches to attack person detection}},
url = {http://arxiv.org/abs/1904.08653},
year = {2019}
}
@article{Cohen2019,
abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the â„“2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between â„“2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with â„“2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified â„“2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification. Code and models arc available at http://github.com/locuslab/smoothing.},
archivePrefix = {arXiv},
arxivId = {1902.02918},
author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, J. Zico},
eprint = {1902.02918},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
pages = {2323--2356},
title = {{Certified adversarial robustness via randomized smoothing}},
volume = {2019-June},
year = {2019}
}
@article{Koh2017,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions - a classic technique from robust statistics - to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Koh, Pang Wei and Liang, Percy},
eprint = {1703.04730},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2976--2987},
title = {{Understanding black-box predictions via influence functions}},
volume = {4},
year = {2017}
}
@article{Che2019,
abstract = {Deep neural networks are vulnerable to adversarial attacks.},
archivePrefix = {arXiv},
arxivId = {1911.07682},
author = {Che, Zhaohui and Borji, Ali and Zhai, Guangtao and Ling, Suiyi and Li, Jing and Callet, Patrick Le},
eprint = {1911.07682},
journal = {arXiv preprint arXiv:1911.07682},
month = {nov},
title = {{A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories}},
url = {http://arxiv.org/abs/1911.07682},
year = {2019}
}
@inproceedings{Rony2018,
abstract = {Research on adversarial examples in computer vision tasks has shown that small, often imperceptible changes to an image can induce misclassification, which has security implications for a wide range of image processing systems. Considering $L_2$ norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations. In this paper, an efficient approach is proposed to generate gradient-based attacks that induce misclassifications with low $L_2$ norm, by decoupling the direction and the norm of the adversarial perturbation that is added to the image. Experiments conducted on the MNIST, CIFAR-10 and ImageNet datasets indicate that our attack achieves comparable results to the state-of-the-art (in terms of $L_2$ norm) with considerably fewer iterations (as few as 100 iterations), which opens the possibility of using these attacks for adversarial training. Models trained with our attack achieve state-of-the-art robustness against white-box gradient-based $L_2$ attacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense when the attacks are limited to a maximum norm.},
archivePrefix = {arXiv},
arxivId = {1811.09600},
author = {Rony, J{\'{e}}r{\^{o}}me and Hafemann, Luiz G. and Oliveira, Luiz S. and Ayed, Ismail Ben and Sabourin, Robert and Granger, Eric},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
eprint = {1811.09600},
pages = {4322--4330},
title = {{Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses}},
url = {http://arxiv.org/abs/1811.09600},
year = {2018}
}
@inproceedings{Song2018,
abstract = {Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.},
archivePrefix = {arXiv},
arxivId = {1805.07894},
author = {Song, Yang and Kushman, Nate and Shu, Rui and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1805.07894},
issn = {10495258},
pages = {8312--8323},
title = {{Constructing unrestricted adversarial examples with generative models}},
volume = {2018-Decem},
year = {2018}
}
@article{RN61,
abstract = {Deep neural networks are powerful and popular learning models that achieve state-of-the-art pattern recognition performance on many computer vision, speech, and language processing tasks. However, these networks have also been shown susceptible to carefully crafted adversarial perturbations which force misclassification of the inputs. Adversarial examples enable adversaries to subvert the expected system behavior leading to undesired consequences and could pose a security risk when these systems are deployed in the real world. In this work, we focus on deep convolutional neural networks and demonstrate that adversaries can easily craft adversarial examples even without any internal knowledge of the target network. Our attacks treat the network as an oracle (black-box) and only assume that the output of the network can be observed on the probed inputs. Our first attack is based on a simple idea of adding perturbation to a randomly selected single pixel or a small set of them. We then improve the effectiveness of this attack by carefully constructing a small set of pixels to perturb by using the idea of greedy local-search. Our proposed attacks also naturally extend to a stronger notion of misclassification. Our extensive experimental results illustrate that even these elementary attacks can reveal a deep neural network's vulnerabilities. The simplicity and effectiveness of our proposed schemes mean that they could serve as a litmus test for designing robust networks.},
archivePrefix = {arXiv},
arxivId = {1612.06299},
author = {Narodytska, Nina and Kasiviswanathan, Shiva Prasad},
eprint = {1612.06299},
journal = {ArXiv},
month = {dec},
title = {{Simple Black-Box Adversarial Perturbations for Deep Networks}},
url = {http://arxiv.org/abs/1612.06299},
year = {2016}
}
@inproceedings{Uesato2018,
abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses. Copyright 2018 by the author(s).},
author = {Uesato, Jonathan and O'Donoghue, Brendan and {Van Den Oord}, Aaron and Kohli, Pushmeet},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
isbn = {9781510867963},
pages = {5032--5041},
title = {{Adversarial risk and the dangers of evaluating against weak attacks}},
volume = {80},
year = {2018}
}
@article{Bishop1995,
abstract = {It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise.},
author = {Bishop, Chris M.},
doi = {10.1162/neco.1995.7.1.108},
issn = {0899-7667},
journal = {Neural Computation},
number = {1},
pages = {108--116},
title = {{Training with Noise is Equivalent to Tikhonov Regularization}},
volume = {7},
year = {1995}
}
@article{Tu2018,
abstract = {Recent studies have shown that adversarial examples in state-of-the-art image classifiers trained by deep neural networks (DNN) can be easily generated when the target model is transparent to an attacker, known as the white-box setting. However, when attacking a deployed machine learning service, one can only acquire the input-output correspondences of the target model; this is the so-called black-box attack setting. The major drawback of existing black-box attacks is the need for excessive model queries, which may give a false sense of model robustness due to inefficient query designs. To bridge this gap, we propose a generic framework for query-efficient black-box attacks. Our framework, AutoZOOM, which is short for Autoencoder-based Zeroth Order Optimization Method, has two novel building blocks towards efficient black-box attacks: (i) an adaptive random gradient estimation strategy to balance query counts and distortion, and (ii) an autoencoder that is either trained offline with unlabeled data or a bilinear resizing operation for attack acceleration. Experimental results suggest that, by applying AutoZOOM to a state-of-the-art black-box attack (ZOO), a significant reduction in model queries can be achieved without sacrificing the attack success rate and the visual quality of the resulting adversarial examples. In particular, when compared to the standard ZOO method, AutoZOOM can consistently reduce the mean query counts in finding successful adversarial examples (or reaching the same distortion level) by at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel insights on adversarial robustness.},
archivePrefix = {arXiv},
arxivId = {1805.11770},
author = {Tu, Chun-Chen and Ting, Paishun and Chen, Pin-Yu and Liu, Sijia and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui and Cheng, Shin-Ming},
eprint = {1805.11770},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {may},
pages = {742--749},
title = {{AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks}},
url = {http://arxiv.org/abs/1805.11770},
volume = {33},
year = {2018}
}
@article{Guo2018,
abstract = {This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.},
archivePrefix = {arXiv},
arxivId = {1711.00117},
author = {Guo, Chuan and Rana, Mayank and Ciss{\'{e}}, Moustapha and {Van Der Maaten}, Laurens},
eprint = {1711.00117},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
number = {1},
pages = {1--12},
title = {{Countering adversarial images using input transformations}},
year = {2018}
}
@article{Tramer2018,
abstract = {Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks (Kurakin et al., 2017c).},
archivePrefix = {arXiv},
arxivId = {1705.07204},
author = {Tram{\`{e}}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
eprint = {1705.07204},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--20},
title = {{Ensemble adversarial training: Attacks and defenses}},
year = {2018}
}
@article{Fu2018,
abstract = {Zero-Shot Learning (ZSL) for visual recognition is typically achieved by exploiting a semantic embedding space. In such a space, both seen and unseen class labels as well as image features can be embedded so that the similarity among them can be measured directly. In this work, we consider that the key to effective ZSL is to compute an optimal distance metric in the semantic embedding space. Existing ZSL works employ either euclidean or cosine distances. However, in a high-dimensional space where the projected class labels (prototypes) are sparse, these distances are suboptimal, resulting in a number of problems including hubness and domain shift. To overcome these problems, a novel manifold distance computed on a semantic class prototype graph is proposed which takes into account the rich intrinsic semantic structure, i.e., semantic manifold, of the class prototype distribution. To further alleviate the domain shift problem, a new regularisation term is introduced into a ranking loss based embedding model. Specifically, the ranking loss objective is regularised by unseen class prototypes to prevent the projected object features from being biased towards the seen prototypes. Extensive experiments on four benchmarks show that our method significantly outperforms the state-of-the-art.},
author = {Fu, Zhenyong and Xiang, Tao and Kodirov, Elyor and Gong, Shaogang},
doi = {10.1109/TPAMI.2017.2737007},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Zero-shot learning,absorbing Markov chain process,class prototype graph,hubness,semantic embedding,semantic manifold},
number = {8},
pages = {2009--2022},
publisher = {IEEE},
title = {{Zero-Shot Learning on Semantic Class Prototype Graph}},
volume = {40},
year = {2018}
}
@inproceedings{Dabouei2019a,
abstract = {The state-of-the-art performance of deep learning algorithms has led to a considerable increase in the utilization of machine learning in security-sensitive and critical applications. However, it has recently been shown that a small and carefully crafted perturbation in the input space can completely fool a deep model. In this study, we explore the extent to which face recognition systems are vulnerable to geometrically-perturbed adversarial faces. We propose a fast landmark manipulation method for generating adversarial faces, which is approximately 200 times faster than the previous geometric attacks and obtains 99.86% success rate on the state-of-the-art face recognition models. To further force the generated samples to be natural, we introduce a second attack constrained on the semantic structure of the face which has the half speed of the first attack with the success rate of 99.96%. Both attacks are extremely robust against the state-of-the-art defense methods with the success rate of equal or greater than 53.59%. Code is available at https://github.com/alldbi/FLM.},
archivePrefix = {arXiv},
arxivId = {1809.08999},
author = {Dabouei, Ali and Soleymani, Sobhan and Dawson, Jeremy and Nasrabadi, Nasser M.},
booktitle = {Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019},
doi = {10.1109/WACV.2019.00215},
eprint = {1809.08999},
isbn = {9781728119755},
month = {jan},
pages = {1979--1988},
publisher = {IEEE},
title = {{Fast Geometrically-Perturbed Adversarial Faces}},
url = {https://ieeexplore.ieee.org/document/8658241/},
year = {2019}
}
@article{Abadi2016,
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
archivePrefix = {arXiv},
arxivId = {1607.00133},
author = {Abadi, Mart{\'{i}}n and McMahan, H. Brendan and Chu, Andy and Mironov, Ilya and Zhang, Li and Goodfellow, Ian and Talwar, Kunal},
doi = {10.1145/2976749.2978318},
eprint = {1607.00133},
isbn = {9781450341394},
issn = {15437221},
journal = {Proceedings of the ACM Conference on Computer and Communications Security},
number = {Ccs},
pages = {308--318},
title = {{Deep learning with differential privacy}},
volume = {24-28-Octo},
year = {2016}
}
@article{Attack2020,
author = {Attack, Shadow},
pages = {1--16},
title = {{Certified Defenses : S Emantic Adver - Sarial Examples With Spoofed Robustness Cer -}},
volume = {2},
year = {2020}
}
@inproceedings{10.1007/978-3-030-01258-8_10,
abstract = {Existing black-box attacks on deep neural networks (DNNs) have largely focused on transferability, where an adversarial instance generated for a locally trained model can â€œtransferâ€ to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model's class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We carry out a thorough comparative evaluation of black-box attacks and show that Gradient Estimation attacks achieve attack success rates similar to state-of-the-art white-box attacks on the MNIST and CIFAR-10 datasets. We also apply the Gradient Estimation attacks successfully against real-world classifiers hosted by Clarifai. Further, we evaluate black-box attacks against state-of-the-art defenses based on adversarial training and show that the Gradient Estimation attacks are very effective even against these defenses.},
address = {Cham},
author = {Bhagoji, Arjun Nitin and He, Warren and Li, Bo and Song, Dawn},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01258-8_10},
editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
isbn = {9783030012571},
issn = {16113349},
keywords = {Adversarial examples,Black-box attacks,Deep neural networks,Image classification},
pages = {158--174},
publisher = {Springer International Publishing},
title = {{Practical black-box attacks on deep neural networks using efficient query mechanisms}},
volume = {11216 LNCS},
year = {2018}
}
@article{Cheng2019,
abstract = {We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1906.06919},
author = {Cheng, Shuyu and Dong, Yinpeng and Pang, Tianyu and Su, Hang and Zhu, Jun},
eprint = {1906.06919},
title = {{Improving Black-box Adversarial Attacks with a Transfer-based Prior}},
url = {http://arxiv.org/abs/1906.06919},
year = {2019}
}
@article{Chen2017ZOOZO,
abstract = {Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack (e.g., Carlini and Wagner's attack) and significantly outperforms existing black-box attacks via substitute models.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1708.03999},
author = {Chen, Pin Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho Jui},
doi = {10.1145/3128572.3140448},
eprint = {1708.03999},
isbn = {9781450352024},
journal = {AISec 2017 - Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
keywords = {Adversarial learning,Black-box attack,Deep learning,Neural network,Substitute model},
month = {aug},
pages = {15--26},
publisher = {ACM Press},
title = {{ZOO: Zeroth order optimization based black-box atacks to deep neural networks without training substitute models}},
url = {http://dl.acm.org/citation.cfm?doid=3128572.3140448 http://dx.doi.org/10.1145/3128572.3140448},
year = {2017}
}
@article{Chao2019,
abstract = {As a unique biometric feature that can be recognized at a distance, gait has broad applications in crime prevention, forensic identification and social security. To portray a gait, existing gait recognition methods utilize either a gait template, where temporal information is hard to preserve, or a gait sequence, which must keep unnecessary sequential constraints and thus loses the flexibility of gait recognition. In this paper we present a novel perspective, where a gait is regarded as a set consisting of independent frames. We propose a new network named GaitSet to learn identity information from the set. Based on the set perspective, our method is immune to permutation of frames, and can naturally integrate frames from different videos which have been filmed under different scenarios, such as diverse viewing angles, different clothes/carrying conditions. Experiments show that under normal walking conditions, our single-model method achieves an average rank-1 accuracy of 95.0% on the CASIA-B gait dataset and an 87.1% accuracy on the OU-MVLP gait dataset. These results represent new state-of-the-art recognition accuracy. On various complex scenarios, our model exhibits a significant level of robustness. It achieves accuracies of 87.2% and 70.4% on CASIA-B under bag-carrying and coat-wearing walking conditions, respectively. These outperform the existing best methods by a large margin. The method presented can also achieve a satisfactory accuracy with a small number of frames in a test sample, e.g., 82.5% on CASIA-B with only 7 frames. The source code has been released at https://github.com/AbnerHqC/GaitSet.},
author = {Chao, Hanqing and He, Yiwei and Zhang, Junping and Feng, Jianfeng},
doi = {10.1609/aaai.v33i01.33018126},
issn = {2374-3468},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {jul},
pages = {8126--8133},
title = {{GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/4821},
volume = {33},
year = {2019}
}
@inproceedings{Das2018a,
abstract = {The rapidly growing body of research in adversarial machine learning has demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarially generated images. This underscores the urgent need for practical defense techniques that can be readily deployed to combat attacks in real-time. Observing that many attack strategies aim to perturb image pixels in ways that are visually imperceptible, we place JPEG compression at the core of our proposed Shield defense framework, utilizing its capability to effectively â€œcompress awayâ€ such pixel manipulation. To immunize a DNN model from artifacts introduced by compression, Shield â€œvaccinatesâ€ the model by retraining it with compressed images, where different compression levels are applied to generate multiple vaccinated models that are ultimately used together in an ensemble defense. On top of that, Shield adds an additional layer of protection by employing randomization at test time that compresses different regions of an image using random compression levels, making it harder for an adversary to estimate the transformation performed. This novel combination of vaccination, ensembling, and randomization makes Shield a fortified multi-pronged defense. We conducted extensive, large-scale experiments using the ImageNet dataset, and show that our approaches eliminate up to 98% of gray-box attacks delivered by strong adversarial techniques such as Carlini-Wagner's L2 attack and DeepFool. Our approaches are fast and work without requiring knowledge about the model.},
archivePrefix = {arXiv},
arxivId = {1802.06816},
author = {Das, Nilaksh and Shanbhogue, Madhuri and Chen, Shang Tse and Hohman, Fred and Li, Siwei and Chen, Li and Kounavis, Michael E. and Chau, Duen Horng},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3219819.3219910},
eprint = {1802.06816},
isbn = {9781450355520},
keywords = {Adversarial machine learning,Deep learning,Ensemble defense,JPEG compression,Machine learning security},
month = {jul},
pages = {196--204},
publisher = {Association for Computing Machinery},
title = {{Shield: Fast, practical defense and vaccination for deep learning using JPEG compression}},
year = {2018}
}
@inproceedings{RN51,
abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassi fied by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
booktitle = {ASIA CCS 2017 - Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security},
doi = {10.1145/3052973.3053009},
eprint = {1602.02697},
isbn = {9781450349444},
pages = {506--519},
publisher = {ACM Press},
title = {{Practical black-box attacks against machine learning}},
type = {Conference Proceedings},
url = {http://dl.acm.org/citation.cfm?doid=3052973.3053009},
year = {2017}
}
@inproceedings{Athalye2018,
abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that causc obfuscated gradients appear to defeat iterative optimization- based attacks, wc find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types Qf obfuscated gradients we discover, wc develop attack techniques to overcome it. In a case study, examining non- certified white-box-secure defenses at ICLR 2018. we find obfuscated gradients arc a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
archivePrefix = {arXiv},
arxivId = {1802.00420},
author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.00420},
isbn = {9781510867963},
pages = {436--448},
title = {{Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples}},
volume = {1},
year = {2018}
}
@article{RN52,
abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
archivePrefix = {arXiv},
arxivId = {1611.02770},
author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
eprint = {1611.02770},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {nov},
title = {{Delving into transferable adversarial examples and black-box attacks}},
url = {http://arxiv.org/abs/1611.02770},
year = {2019}
}
@article{Wong2018,
abstract = {We propose a method to learn deep ReLU-based classifiers that are provably robust against normbounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded â„“âˆž norm less than Ïµ = 0.1), and code for all experiments is available at http://github.com/locuslab/convex-adversarial.},
archivePrefix = {arXiv},
arxivId = {1711.00851},
author = {Wong, Eric and Kolter, J. Zico},
eprint = {1711.00851},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {8405--8423},
title = {{Provable defenses against adversarial examples via the convex outer adversarial polytope}},
volume = {12},
year = {2018}
}
@article{Kwon2018,
abstract = {Deep neural networks (DNNs) are widely used for image recognition, speech recognition, pattern analysis, and intrusion detection. Recently, the adversarial example attack, in which the input data are only slightly modified, although not an issue for human interpretation, is a serious threat to a DNN as an attack as it causes the machine to misinterpret the data. The adversarial example attack has been receiving considerable attention owing to its potential threat to machine learning. It is divided into two categories: targeted adversarial example and untargeted adversarial example. The untargeted adversarial example happens when machines misclassify an object into an incorrect class. In contrast, the targeted adversarial example attack causes machines to misinterpret the image as the attacker's desired class. Thus, the latter is a more elaborate and powerful attack than the former. The existing targeted adversarial example is a single targeted attack that allows only one class to be recognized. However, in some cases, a multi-targeted adversarial example can be useful for an attacker to make multiple models recognize a single original image as different classes. For example, an attacker can use a single road sign generated by a multi-targeted adversarial example scheme to make model A recognize it as a stop sign and model B recognize it as a left turn, whereas a human might recognize it as a right turn. Therefore, in this paper, we propose a multi-targeted adversarial example that attacks multiple models within each target class with a single modified image. To produce such examples, we carried out a transformation to maximize the probability of different target classes by multiple models. We used the MNIST datasets and TensorFlow library for our experiment. The experimental results showed that the proposed scheme for generating a multi-targeted adversarial example achieved a 100% attack success rate.},
author = {Kwon, Hyun and Kim, Yongchul and Park, Ki Woong and Yoon, Hyunsoo and Choi, Daeseon},
doi = {10.1109/ACCESS.2018.2866197},
issn = {21693536},
journal = {IEEE Access},
title = {{Multi-Targeted Adversarial Example in Evasion Attack on Deep Neural Network}},
year = {2018}
}
@article{Wiyatno2019,
abstract = {Recent research has found that many families of machine learning models are vulnerable to adversarial examples: inputs that are specifically designed to cause the target model to produce erroneous outputs. In this survey, we focus on machine learning models in the visual domain, where methods for generating and detecting such examples have been most extensively studied. We explore a variety of adversarial attack methods that apply to image-space content, real world adversarial attacks, adversarial defenses, and the transferability property of adversarial examples. We also discuss strengths and weaknesses of various methods of adversarial attack and defense. Our aim is to provide an extensive coverage of the field, furnishing the reader with an intuitive understanding of the mechanics of adversarial attack and defense mechanisms and enlarging the community of researchers studying this fundamental set of problems.},
archivePrefix = {arXiv},
arxivId = {1911.05268},
author = {Wiyatno, Rey Reza and Xu, Anqi and Dia, Ousmane and de Berker, Archy},
eprint = {1911.05268},
journal = {arXiv preprint arXiv:1911.05268},
month = {nov},
title = {{Adversarial Examples in Modern Machine Learning: A Review}},
url = {http://arxiv.org/abs/1911.05268},
year = {2019}
}
@inproceedings{Deng2018,
abstract = {One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that enhance discriminative power. Centre loss penalises the distance between the deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in an angular space and penalises the angles between the deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to the exact correspondence to the geodesic distance on the hypersphere. We present arguably the most extensive experimental evaluation of all the recent state-of-the-art face recognition methods on over 10 face recognition benchmarks including a new large-scale image database with trillion level of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state-of-the-art and can be easily implemented with negligible computational overhead. We release all refined training data, training codes, pre-trained models and training logs, which will help reproduce the results in this paper.},
archivePrefix = {arXiv},
arxivId = {1801.07698},
author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
eprint = {1801.07698},
pages = {4690--4699},
title = {{ArcFace: Additive Angular Margin Loss for Deep Face Recognition}},
url = {http://arxiv.org/abs/1801.07698},
year = {2018}
}
@inproceedings{Shafahi2018,
abstract = {Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use â€œclean-labelsâ€; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a â€œwatermarkingâ€ strategy that makes poisoning reliable using multiple (â‰ˆ 50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.},
author = {Shafahi, Ali and {Ronny Huang}, W. and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Poison frogs! Targeted clean-label poisoning attacks on neural networks}},
year = {2018}
}
@article{Pathak2012,
author = {Pathak, Manas A and Raj, Bhiksha},
isbn = {9783319957296},
keywords = {differential privacy,image privacy},
number = {4},
pages = {463--469},
title = {{with Differential Privacy}},
volume = {9},
year = {2012}
}
@article{Raghunathan2018,
abstract = {While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most = 0.1 can cause more than 35% test error.},
archivePrefix = {arXiv},
arxivId = {1801.09344},
author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
eprint = {1801.09344},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--15},
title = {{Certified defenses against adversarial examples}},
year = {2018}
}
@inproceedings{Rozsa2016a,
abstract = {State-of-the-art deep neural networks suffer from a fundamental problem - they misclassify adversarial examples formed by applying small perturbations to inputs. In this paper, we present a new psychometric perceptual adversarial similarity score (PASS) measure for quantifying adversarial images, introduce the notion of hard positive generation, and use a diverse set of adversarial perturbations - not just the closest ones - for data augmentation. We introduce a novel hot/cold approach for adversarial example generation, which provides multiple possible adversarial perturbations for every single image. The perturbations generated by our novel approach often correspond to semantically meaningful image structures, and allow greater flexibility to scale perturbation-amplitudes, which yields an increased diversity of adversarial images. We present adversarial images on several network topologies and datasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNet on the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet that fine-tuning with a diverse set of hard positives improves the robustness of these networks compared to training with prior methods of generating adversarial images.},
archivePrefix = {arXiv},
arxivId = {1605.01775},
author = {Rozsa, Andras and Rudd, Ethan M. and Boult, Terrance E.},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.58},
eprint = {1605.01775},
isbn = {9781467388504},
issn = {21607516},
month = {jun},
pages = {410--417},
publisher = {IEEE},
title = {{Adversarial Diversity and Hard Positive Generation}},
url = {http://ieeexplore.ieee.org/document/7789548/},
year = {2016}
}
@article{Fawzi2018,
abstract = {The goal of this paper is to analyze the intriguing instability of classifiers to adversarial perturbations (Szegedy et al., in: International conference on learning representations (ICLR), 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on two practical classes of classifiers, namely the linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured mathematically by the distinguishability measure). We further show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to d (with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed by Szegedy et al. in the context of neural networks. We finally show experimental results on controlled and real-world data that confirm the theoretical analysis and extend its spirit to more complex classification schemes.},
archivePrefix = {arXiv},
arxivId = {1502.02590},
author = {Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
doi = {10.1007/s10994-017-5663-3},
eprint = {1502.02590},
issn = {15730565},
journal = {Machine Learning},
keywords = {Adversarial examples,Classification robustness,Deep networks,Instability,Random noise},
number = {3},
pages = {481--508},
publisher = {Springer US},
title = {{Analysis of classifiers' robustness to adversarial perturbations}},
volume = {107},
year = {2018}
}
@article{Zhang2019,
abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which wc won the 1st place out of -2,000 submissions, surpassing the runner-up approach by 11.41% in terms of mean {\pounds}2 perturbation distance.},
archivePrefix = {arXiv},
arxivId = {1901.08573},
author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P. and Ghaoui, Laurent El and Jordan, Michael I.},
eprint = {1901.08573},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
pages = {12907--12929},
title = {{Theoretically principled trade-off between robustness and accuracy}},
volume = {2019-June},
year = {2019}
}
@article{chen2019hopskipjumpattack,
abstract = {The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for $\ell_2$ and $\ell_\infty$ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than Boundary Attack. It also achieves competitive performance in attacking several widely-used defense mechanisms. (HopSkipJumpAttack was named Boundary Attack++ in a previous version of the preprint.)},
archivePrefix = {arXiv},
arxivId = {1904.02144},
author = {Chen, Jianbo and Jordan, Michael I. and Wainwright, Martin J.},
eprint = {1904.02144},
journal = {ArXiv},
month = {apr},
title = {{HopSkipJumpAttack: A Query-Efficient Decision-Based Attack}},
url = {http://arxiv.org/abs/1904.02144},
volume = {3},
year = {2019}
}
@article{He2019a,
abstract = {Deep learning has gained tremendous success and great popularity in the past few years. However, recent research found that it is suffering several inherent weaknesses, which can threaten the security and privacy of the stackholders. Deep learning's wide use further magnifies the caused consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few is clear about how these weaknesses are incurred and how effective are these attack approaches in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we are devoted to undertaking a comprehensive investigation on attacks towards deep learning, and extensively evaluating these attacks in multiple views. In particular, we focus on four types of attacks associated with security and privacy of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Many pivot metrics are devised for evaluating the attack approaches, by which we perform a quantitative and qualitative analysis. From the analysis, we have identified significant and indispensable factors in an attack vector, \eg, how to reduce queries to target models, what distance used for measuring perturbation. We spot light on 17 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant researchers in this area.},
archivePrefix = {arXiv},
arxivId = {1911.12562},
author = {He, Yingzhe and Meng, Guozhu and Chen, Kai and Hu, Xingbo and He, Jinwen},
eprint = {1911.12562},
journal = {arXiv preprint arXiv:1911.12562},
month = {nov},
title = {{Towards Privacy and Security of Deep Learning Systems: A Survey}},
url = {http://arxiv.org/abs/1911.12562},
year = {2019}
}
@inproceedings{eykholt2017robust,
abstract = {Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations. Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm, Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. With a perturbation in the form of only black and white stickers, we attack a real stop sign, causing targeted misclassification in 100% of the images obtained in lab settings, and in 84.8% of the captured video frames obtained on a moving vehicle (field test) for the target classifier.},
author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00175},
isbn = {978-1-5386-6420-9},
issn = {10636919},
month = {jun},
pages = {1625--1634},
publisher = {IEEE},
title = {{Robust Physical-World Attacks on Deep Learning Visual Classification}},
url = {https://ieeexplore.ieee.org/document/8578273/},
year = {2018}
}
@article{Hang2020,
abstract = {Deep learning (DL) models, e.g., state-of-the-art convolutional neural networks (CNNs), have been widely applied into security sensitivity tasks, such as face payment, security monitoring, automated driving, etc. Then their vulnerability analysis is an emergent topic, especially for black-box attacks, where adversaries do not know the model internal architectures or training parameters. In this paper, two types of ensemble-based black-box attack strategies, selective cascade ensemble strategy (SCES) and stack parallel ensemble strategy (SPES), are proposed to explore the vulnerability of DL system and potential factors that contribute to the high-efficiency attacks are explored. SCES adopts a boosting structure of ensemble learning and SPES employs a bagging structure. Moreover, two pairwise and non-pairwise diversity measures are adopted to examine the relationship between the diversity in substitutes ensembles and transferability of generated adversarial examples. Experimental results show that proposed ensemble adversarial black-box attack strategies can successfully attack the DL system with some defense mechanism, such as adversarial training and ensemble adversarial training. The experimental results also show the greater the diversity in substitute ensembles enables stronger transferability.},
author = {Hang, Jie and Han, Keji and Chen, Hui and Li, Yun},
doi = {10.1016/j.patcog.2019.107184},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Black-box attack,Diversity,Ensemble adversarial attack,Transferability,Vulnerability},
title = {{Ensemble adversarial black-box attacks against deep learning systems}},
year = {2020}
}
@inproceedings{pmlr-v97-pang19a,
abstract = {Though deep neural networks have achieved significant progress on various tasks, often enhanced by model ensemble, existing high-performance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the outputs, which ignores the interaction among networks. This paper presents a new method that explores the interaction among individual networks to improve robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal predictions of individual members, and present an adaptive diversity promoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examples difficult to transfer among individual members. Our method is computationally efficient and compatible with the defense methods acting on individual networks. Empirical results on various datasets verify that our method can improve adversarial robustness while maintaining state-of-the-art accuracy on normal examples. The authors would like to thank Chongxuan Li for helpful comments. This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008,61673241,61571261), Beijing NSF Project (No. LI72037), DITD Program JCKY2017204B064, Tian-gong Institute for Intelligent Computing, Beijing Academy of Artificial Intelligence (BAAI), NVIDIA NVAIL Program, and the projects from Siemens and Intel.},
address = {Long Beach, California, USA},
archivePrefix = {arXiv},
arxivId = {1901.08846},
author = {Pang, Tianyu and Xu, Kun and Du, Chao and Chen, Ning and Zhu, Jun},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
eprint = {1901.08846},
isbn = {9781510886988},
pages = {8759--8771},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Improving adversarial robustness via promoting ensemble diversity}},
url = {http://proceedings.mlr.press/v97/pang19a.html},
volume = {2019-June},
year = {2019}
}
@article{Karam2015,
abstract = {The varying quality of face images is an important challenge that limits the effectiveness of face recognition technology when applied in real-world applications. Existing face im- age databases do not consider the effect of distortions that commonly occur in real-world environments. This database (QLFW) represents an initial attempt to provide a set of la- beled face images spanning the wide range of quality, from no perceived impairment to strong perceived impairment. Types of impairment include JPEG2000 compression, JPEG compression, additive white noise, Gaussian blur and con- trast change. One goal of this work is to enable automated performance evaluation of face recognition technologies in the presence of different types and levels of visual distortions. This will consequently enable the development of face recog- nition systems that can operate reliably on real-world visual content in the presence of real-world visual distortions.},
author = {Karam, Lina J. and Zhu, Tong},
doi = {10.1117/12.2080393},
isbn = {9781628414844},
issn = {1996756X},
journal = {Human Vision and Electronic Imaging XX},
pages = {93940B},
title = {{Quality labeled faces in the wild (QLFW): a database for studying face recognition in real-world environments}},
volume = {9394},
year = {2015}
}
@inproceedings{ilyas2018prior,
abstract = {We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and we demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less than the current state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1807.07978},
author = {Ilyas, Andrew and Engstrom, Logan and Madry, Aleksander},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1807.07978},
title = {{Prior convictions: Black-box adversarial attacks with bandits and priors}},
url = {https://openreview.net/forum?id=BkMiWhR5K7},
year = {2019}
}
@article{Wang2019,
abstract = {Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models.},
author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and an Tan, Yu and Li, Jin},
doi = {10.1016/j.jpdc.2019.03.003},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Adversarial attack,Adversarial example,Adversarial setting,Machine learning,Security model},
title = {{The security of machine learning in an adversarial setting: A survey}},
year = {2019}
}
@article{RN59,
abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
archivePrefix = {arXiv},
arxivId = {1511.07528},
author = {Papernot, Nicolas and Mcdaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
doi = {10.1109/EuroSP.2016.36},
eprint = {1511.07528},
isbn = {9781509017515},
journal = {Proceedings - 2016 IEEE European Symposium on Security and Privacy, EURO S and P 2016},
month = {mar},
pages = {372--387},
publisher = {IEEE},
title = {{The limitations of deep learning in adversarial settings}},
url = {http://ieeexplore.ieee.org/document/7467366/},
year = {2016}
}
@article{RN46,
abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
archivePrefix = {arXiv},
arxivId = {1607.02533},
author = {Kurakin, Alexey and Goodfellow, Ian J. and Bengio, Samy},
eprint = {1607.02533},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
title = {{Adversarial examples in the physical world}},
year = {2017}
}
@article{Levine2019,
abstract = {Recently, techniques have been developed to provably guarantee the robustness of a classifier to adversarial perturbations of bounded L_1 and L_2 magnitudes by using randomized smoothing: the robust classification is a consensus of base classifications on randomly noised samples where the noise is additive. In this paper, we extend this technique to the L_0 threat model. We propose an efficient and certifiably robust defense against sparse adversarial attacks by randomly ablating input features, rather than using additive noise. Experimentally, on MNIST, we can certify the classifications of over 50% of images to be robust to any distortion of at most 8 pixels. This is comparable to the observed empirical robustness of unprotected classifiers on MNIST to modern L_0 attacks, demonstrating the tightness of the proposed robustness certificate. We also evaluate our certificate on ImageNet and CIFAR-10. Our certificates represent an improvement on those provided in a concurrent work (Lee et al. 2019) which uses random noise rather than ablation (median certificates of 8 pixels versus 4 pixels on MNIST; 16 pixels versus 1 pixel on ImageNet.) Additionally, we empirically demonstrate that our classifier is highly robust to modern sparse adversarial attacks on MNIST. Our classifications are robust, in median, to adversarial perturbations of up to 31 pixels, compared to 22 pixels reported as the state-of-the-art defense, at the cost of a slight decrease (around 2.3%) in the classification accuracy. Code is available at https://github.com/alevine0/randomizedAblation/.},
archivePrefix = {arXiv},
arxivId = {1911.09272},
author = {Levine, Alexander and Feizi, Soheil},
eprint = {1911.09272},
title = {{Robustness Certificates for Sparse Adversarial Attacks by Randomized Ablation}},
url = {http://arxiv.org/abs/1911.09272},
year = {2019}
}
@article{Papernot2016,
abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
archivePrefix = {arXiv},
arxivId = {1605.07277},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
eprint = {1605.07277},
title = {{Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples}},
url = {http://arxiv.org/abs/1605.07277},
year = {2016}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. {\textcopyright} 2006 Massachusetts Institute of Technology.},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
doi = {10.1162/neco.2006.18.7.1527},
issn = {08997667},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
volume = {18},
year = {2006}
}
@article{Moosavi-Dezfooli2017,
abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
archivePrefix = {arXiv},
arxivId = {1705.09554},
author = {Moosavi-Dezfooli, Seyed Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
doi = {10.1109/CVPR.2017.17},
eprint = {1705.09554},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {86--94},
title = {{Universal adversarial perturbations}},
volume = {2017-Janua},
year = {2017}
}
@article{Shamir2019,
abstract = {The existence of adversarial examples in which an imperceptible change in the input can fool well trained neural networks was experimentally discovered by Szegedy et al in 2013, who called them "Intriguing properties of neural networks". Since then, this topic had become one of the hottest research areas within machine learning, but the ease with which we can switch between any two decisions in targeted attacks is still far from being understood, and in particular it is not clear which parameters determine the number of input coordinates we have to change in order to mislead the network. In this paper we develop a simple mathematical framework which enables us to think about this baffling phenomenon from a fresh perspective, turning it into a natural consequence of the geometry of $\mathbb{R}^n$ with the $L_0$ (Hamming) metric, which can be quantitatively analyzed. In particular, we explain why we should expect to find targeted adversarial examples with Hamming distance of roughly $m$ in arbitrarily deep neural networks which are designed to distinguish between $m$ input classes.},
archivePrefix = {arXiv},
arxivId = {1901.10861},
author = {Shamir, Adi and Safran, Itay and Ronen, Eyal and Dunkelman, Orr},
eprint = {1901.10861},
journal = {arXiv preprint arXiv:1901.10861},
month = {jan},
title = {{A Simple Explanation for the Existence of Adversarial Examples with Small Hamming Distance}},
url = {http://arxiv.org/abs/1901.10861},
year = {2019}
}
@article{RN60,
abstract = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
archivePrefix = {arXiv},
arxivId = {1710.08864},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
doi = {10.1109/TEVC.2019.2890858},
eprint = {1710.08864},
issn = {19410026},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Convolutional neural network,differential evolution (DE),image recognition,information security},
month = {oct},
number = {5},
pages = {828--841},
title = {{One Pixel Attack for Fooling Deep Neural Networks}},
url = {https://ieeexplore.ieee.org/document/8601309/},
volume = {23},
year = {2019}
}
@article{Biggio2015,
abstract = {In this article, we review previous work on biometric security under a recent framework proposed in the field of adversarial machine learning. This allows us to highlight novel insights on the security of biometric systems when operating in the presence of intelligent and adaptive attackers that manipulate data to compromise normal system operation. We show how this framework enables the categorization of known and novel vulnerabilities of biometric recognition systems, along with the corresponding attacks, countermeasures, and defense mechanisms. We report two application examples, respectively showing how to fabricate a more effective face spoofing attack, and how to counter an attack that exploits an unknown vulnerability of an adaptive face-recognition system to compromise its face templates.},
author = {Biggio, Battista and Fumera, Giorgio and Russu, Paolo and Didaci, Luca and Roli, Fabio},
doi = {10.1109/MSP.2015.2426728},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {5},
pages = {31--41},
publisher = {IEEE},
title = {{Adversarial biometric recognition: A review on biometric system security from the adversarial machine-learning perspective}},
volume = {32},
year = {2015}
}
@article{Kurakin2018,
abstract = {To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.},
archivePrefix = {arXiv},
arxivId = {1804.00097},
author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy and Dong, Yinpeng and Liao, Fangzhou and Liang, Ming and Pang, Tianyu and Zhu, Jun and Hu, Xiaolin and Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan and Huang, Sangxia and Zhao, Yao and Zhao, Yuzhe and Han, Zhonglin and Long, Junjiajia and Berdibekov, Yerkebulan and Akiba, Takuya and Tokui, Seiya and Abe, Motoki},
doi = {10.1007/978-3-319-94042-7_11},
eprint = {1804.00097},
pages = {195--231},
title = {{Adversarial Attacks and Defences Competition}},
year = {2018}
}
@article{Zhang2019b,
abstract = {Deep neural networks (DNNs) have shown huge superiority over humans in image recognition, speech processing, autonomous vehicles and medical diagnosis. However, recent studies indicate that DNNs are vulnerable to adversarial examples (AEs), which are designed by attackers to fool deep learning models. Different from real examples, AEs can mislead the model to predict incorrect outputs while hardly be distinguished by human eyes, therefore threaten security-critical deep-learning applications. In recent years, the generation and defense of AEs have become a research hotspot in the field of artificial intelligence (AI) security. This article reviews the latest research progress of AEs. First, we introduce the concept, cause, characteristics and evaluation metrics of AEs, then give a survey on the state-of-the-art AE generation methods with the discussion of advantages and disadvantages. After that, we review the existing defenses and discuss their limitations. Finally, future research opportunities and challenges on AEs are prospected.},
archivePrefix = {arXiv},
arxivId = {1809.04790},
author = {Zhang, Jiliang and Li, Chen},
doi = {10.1109/TNNLS.2019.2933524},
eprint = {1809.04790},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--16},
title = {{Adversarial Examples: Opportunities and Challenges}},
url = {https://ieeexplore.ieee.org/document/8842604/},
year = {2019}
}
@inproceedings{Sharif2018a,
abstract = {Much research has been devoted to better understanding adversarial examples, which are specially crafted inputs to machine-learning models that are perceptually similar to benign inputs, but are classified differently (i.e., misclassified). Both algorithms that create adversarial examples and strategies for defending against adversarial examples typically use Lp-norms to measure the perceptual similarity between an adversarial input and its benign original. Prior work has already shown, however, that two images need not be close to each other as measured by an Lp-norm to be perceptually similar. In this work, we show that nearness according to an Lp-norm is not just unnecessary for perceptual similarity, but is also insufficient. Specifically, focusing on datasets (CIFAR10 and MNIST), Lp-norms, and thresholds used in prior work, we show through online user studies that 'adversarial examples' that are closer to their benign counterparts than required by commonly used Lp-norm thresholds can nevertheless be perceptually distinct to humans from the corresponding benign examples. Namely, the perceptual distance between two images that are 'near' each other according to an Lp-norm can be high enough that participants frequently classify the two images as representing different objects or digits. Combined with prior work, we thus demonstrate that nearness of inputs as measured by Lp-norms is neither necessary nor sufficient for perceptual similarity, which has implications for both creating and defending against adversarial examples. We propose and discuss alternative similarity metrics to stimulate future research in the area.},
archivePrefix = {arXiv},
arxivId = {1802.09653},
author = {Sharif, Mahmood and Baue, Lujo and Reite, Michael K.},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2018.00211},
eprint = {1802.09653},
isbn = {9781538661000},
issn = {21607516},
month = {jun},
pages = {1686--1694},
publisher = {IEEE},
title = {{On the suitability of lp-norms for creating and preventing adversarial examples}},
url = {https://ieeexplore.ieee.org/document/8575372/},
volume = {2018-June},
year = {2018}
}
@article{Wu2020,
abstract = {Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising security weakness of skip connections in this paper. Use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high transferability. Our method is termed Skip Gradient Method(SGM). We conduct comprehensive transfer attacks against state-of-the-art DNNs including ResNets, DenseNets, Inceptions, Inception-ResNet, Squeeze-and-Excitation Network (SENet) and robustly trained DNNs. We show that employing SGM on the gradient flow can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, SGM can be easily combined with existing black-box attack techniques, and obtain high improvements over state-of-the-art transferability methods. Our findings not only motivate new research into the architectural vulnerability of DNNs, but also open up further challenges for the design of secure DNN architectures.},
archivePrefix = {arXiv},
arxivId = {2002.05990},
author = {Wu, Dongxian and Wang, Yisen and Xia, Shu-Tao and Bailey, James and Ma, Xingjun},
eprint = {2002.05990},
pages = {1--15},
title = {{Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets}},
url = {http://arxiv.org/abs/2002.05990},
year = {2020}
}
@article{brown2017adversarial,
author = {Brown, Tom B and Man{\'{e}}, Dandelion and Roy, Aurko and Abadi, Mart\'\in and Gilmer, Justin},
journal = {arXiv preprint arXiv:1712.09665},
title = {{Adversarial patch}},
year = {2017}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@inproceedings{Salman2019,
abstract = {Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to $\ell_2$-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably $\ell_2$-robust classifiers by a significant margin on ImageNet and CIFAR-10, establishing the state-of-the-art for provable $\ell_2$-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at http://github.com/Hadisalman/smoothing-adversarial .},
archivePrefix = {arXiv},
arxivId = {1906.04584},
author = {Salman, Hadi and Yang, Greg and Li, Jerry and Zhang, Pengchuan and Zhang, Huan and Razenshteyn, Ilya and Bubeck, Sebastien},
booktitle = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019)},
eprint = {1906.04584},
pages = {11289----11300},
title = {{Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers}},
url = {http://arxiv.org/abs/1906.04584},
year = {2019}
}
@article{Samangouei2018,
abstract = {In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies.},
archivePrefix = {arXiv},
arxivId = {1805.06605},
author = {Samangouei, Pouya and Kabkab, Maya and Chellappa, Rama},
eprint = {1805.06605},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
number = {3},
title = {{Defense-Gan: Protecting classifiers against adversarial attacks using generative models}},
year = {2018}
}
@article{Liao2018,
abstract = {Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin.1},
archivePrefix = {arXiv},
arxivId = {1712.02976},
author = {Liao, Fangzhou and Liang, Ming and Dong, Yinpeng and Pang, Tianyu and Hu, Xiaolin and Zhu, Jun},
doi = {10.1109/CVPR.2018.00191},
eprint = {1712.02976},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {Figure 1},
pages = {1778--1787},
title = {{Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser}},
year = {2018}
}
@article{RN44,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
eprint = {1312.6199},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
month = {dec},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2014}
}
@article{Dumoulin2019,
abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
archivePrefix = {arXiv},
arxivId = {1606.00704},
author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
eprint = {1606.00704},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
pages = {1--18},
title = {{Adversarially learned inference}},
year = {2019}
}
@article{Gilmer2018,
abstract = {State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size O(1/d). Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of machine learning models to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1801.02774},
author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
eprint = {1801.02774},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
title = {{Adversarial spheres}},
year = {2018}
}
@article{Papernot2018,
abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive - new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, {\`{a}} la PAC theory, will foster a science of security and privacy in ML.},
author = {Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael P.},
doi = {10.1109/EuroSP.2018.00035},
isbn = {9781538642276},
journal = {Proceedings - 3rd IEEE European Symposium on Security and Privacy, EURO S and P 2018},
keywords = {machine learning,privacy,security},
pages = {399--414},
publisher = {IEEE},
title = {{SoK: Security and Privacy in Machine Learning}},
year = {2018}
}
@article{Kariyappa2019,
abstract = {Deep Neural Networks are vulnerable to adversarial attacks even in settings where the attacker has no direct access to the model being attacked. Such attacks usually rely on the principle of transferability, whereby an attack crafted on a surrogate model tends to transfer to the target model. We show that an ensemble of models with misaligned loss gradients can provide an effective defense against transfer-based attacks. Our key insight is that an adversarial example is less likely to fool multiple models in the ensemble if their loss functions do not increase in a correlated fashion. To this end, we propose Diversity Training, a novel method to train an ensemble of models with uncorrelated loss functions. We show that our method significantly improves the adversarial robustness of ensembles and can also be combined with existing methods to create a stronger defense.},
archivePrefix = {arXiv},
arxivId = {1901.09981},
author = {Kariyappa, Sanjay and Qureshi, Moinuddin K.},
eprint = {1901.09981},
journal = {arXiv preprint arXiv:1901.09981},
month = {jan},
title = {{Improving Adversarial Robustness of Ensembles with Diversity Training}},
url = {http://arxiv.org/abs/1901.09981},
year = {2019}
}
@article{Wittel2004,
abstract = {The efforts of anti-spammers and spammers has often been described as an arms race. As we devise new ways to stem the flood of bulk mail, spammers respond by working their way around the new mechanisms. Their attempts to bypass spam filters illustrates this struggle. Spammers have tried many things from using HTML layout tricks, letter substitution, to adding random data. While at times their attacks are clever, they have yet to work strongly against the statistical nature that drives many filtering systems. The challenges in successfully developing such an attack are great as the variety of filtering systems makes it less likely that a single attack can work against all of them. Here, we examine the general attack methods spammers use, along with challenges faced by developers and spammers. We also demonstrate an attack that, while easy to implement, attempts to more strongly work against the statistical nature behind filters.},
author = {Wittel, Gregory L and Wu, S Felix},
journal = {CEAS: First Conference on Email and Anti-Spam},
title = {{On Attacking Statistical Spam Filters}},
year = {2004}
}
@article{RN57,
abstract = {Recent studies show that widely used deep neural networks (DNNs) are vulnerable to carefully crafted adversarial examples. Many advanced algorithms have been proposed to generate adversarial examples by leveraging the Lp distance for penalizing perturbations. Researchers have explored different defense methods to defend against such adversarial attacks. While the effectiveness of Lp distance as a metric of perceptual quality remains an active research area, in this paper we will instead focus on a different type of perturbation, namely spatial transformation, as opposed to manipulating the pixel values directly as in prior works. Perturbations generated through spatial transformation could result in large Lp distance measures, but our extensive experiments show that such spatially transformed adversarial examples are perceptually realistic and more difficult to defend against with existing defense systems. This potentially provides a new direction in adversarial example generation and the design of corresponding defenses. We visualize the spatial transformation based perturbation for different examples and show that our technique can produce realistic adversarial examples with smooth image deformation. Finally, we visualize the attention of deep networks with different types of adversarial examples to better understand how these examples are interpreted.},
archivePrefix = {arXiv},
arxivId = {1801.02612},
author = {Xiao, Chaowei and Zhu, Jun Yan and Li, Bo and He, Warren and Liu, Mingyan and Song, Dawn},
eprint = {1801.02612},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {jan},
pages = {1--29},
title = {{Spatially transformed adversarial examples}},
url = {http://arxiv.org/abs/1801.02612},
year = {2018}
}
@article{RN49,
abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1608.04644},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1109/SP.2017.49},
eprint = {1608.04644},
isbn = {9781509055326},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
month = {may},
pages = {39--57},
publisher = {IEEE},
title = {{Towards Evaluating the Robustness of Neural Networks}},
url = {http://ieeexplore.ieee.org/document/7958570/},
year = {2017}
}
@inproceedings{Li2019,
abstract = {Recent studies have highlighted that deep neural networks (DNNs) are vulnerable to adversarial examples. In this paper, we improve the robustness of DNNs by utilizing techniques of Distance Metric Learning. Specifically, we incorporate Triplet Loss, one of the most popular Distance Metric Learning methods, into the framework of adversarial training. Our proposed algorithm, Adversarial Training with Triplet Loss (AT2L), substitutes the adversarial example against the current model for the anchor of triplet loss to effectively smooth the classification boundary. Furthermore, we propose an ensemble version of AT2L, which aggregates different attack methods and model structures for better defense effects. Our empirical studies verify that the proposed approach can significantly improve the robustness of DNNs without sacrificing accuracy. Finally, we demonstrate that our specially designed triplet loss can also be used as a regularization term to enhance other defense methods.},
archivePrefix = {arXiv},
arxivId = {1905.11713},
author = {Li, Pengcheng and Yi, Jinfeng and Zhou, Bowen and Zhang, Lijun},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2019/403},
eprint = {1905.11713},
isbn = {9780999241141},
issn = {10450823},
title = {{Improving the robustness of deep neural networks via adversarial training with triplet loss}},
year = {2019}
}
@article{Tramer2017,
abstract = {Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model. In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large ($\sim$25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability. In the first quantitative analysis of the similarity of different models' decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.},
archivePrefix = {arXiv},
arxivId = {1704.03453},
author = {Tram{\`{e}}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
eprint = {1704.03453},
pages = {1--15},
title = {{The Space of Transferable Adversarial Examples}},
url = {http://arxiv.org/abs/1704.03453},
year = {2017}
}
@article{Cisse2017,
abstract = {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than 1. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.},
archivePrefix = {arXiv},
arxivId = {1704.08847},
author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
eprint = {1704.08847},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {1423--1432},
title = {{Parseval networks: Improving robustness to adversarial examples}},
volume = {2},
year = {2017}
}
@article{RN56,
abstract = {While deep neural networks have proven to be a powerful tool for many recognition and classification tasks, their stability properties are still not well understood. In the past, image classifiers have been shown to be vulnerable to so-called adversarial attacks, which are created by additively perturbing the correctly classified image. In this paper, we propose the ADef algorithm to construct a different kind of adversarial attack created by iteratively applying small deformations to the image, found through a gradient descent step. We demonstrate our results on MNIST with convolutional neural networks and on ImageNet with Inception-v3 and ResNet-101.},
archivePrefix = {arXiv},
arxivId = {1804.07729},
author = {Alaifari, Rima and Gauksson, Tandri and Alberti, Giovanni S.},
eprint = {1804.07729},
journal = {7th International Conference on Learning Representations, ICLR 2019},
month = {apr},
title = {{ADEF: An iterative algorithm to construct adversarial deformations}},
url = {http://arxiv.org/abs/1804.07729},
year = {2019}
}
