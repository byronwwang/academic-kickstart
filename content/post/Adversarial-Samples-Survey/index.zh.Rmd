---
title: 视觉对抗样本生成技术概述
authors:
  - Wei Wang
  - Jing Dong
  - Ziwen He
date: '2020-03-04'
tags:
  - adversarial samples
  - survey
lastmod: '2020-03-04T21:06:49+08:00'
image:
  caption: ''
  focal_point: ''
  preview_only: false
featured: false
draft: false
weight: 2
csl: ieee.csl #no csl file is needed for biblatex
bibliography: ref.bib
biblio-title: 参考文献
link-citations: yes #for html
---

# 引言 {#intro}

随着深度学习技术[1]的快速发展，其在公共安全、金融、医疗、娱乐等多个应用领域都取得了巨大成功，人工智能技术再次迎来蓬勃发展期。以人脸识别[2]为代表的人工智能技术的快速落地，尤其是在国内井喷式地发展，给人们生产生活带来极大便利性的同时，也引发了很多安全性问题[@Feynman1963118;@Dirac1953888;@test]。

相较于传统安全问题，如人脸识别系统中的照片攻击、假体攻击等，一类新型的、具有攻击性的对抗样本生成技术近年来受到广泛关注，特别是在视觉数据理解与分析领域。出于视觉数据特有的冗余特性，一系列相关技术被公开发表，并日益完善。鉴于此，人们比以往任何时候都更加关注人工智能系统的安全问题，对抗防御等应对技术也相继被提出。那么，到底是何原因使得人工智能视觉系统易被攻击？是深度神经网络模型的大规模应用带来的问题，还是深度学习技术之前就已存在？是个别系统特有，还是绝大多数系统的共性问题？若是后者，目前是否有普适性的防御技术？希望通过本文的介绍，能够尽可能地回答上述问题。

本文首先针对人工智能机器学习中的安全问题展开宏观介绍及数学描述；接着对具有代表性的对抗样本生成技术分类归纳介绍；最后，针对生物识别系统中人脸识别模型的安全性给出具体分析，以便读者更为直观地了解对抗样本引发的安全问题。

相较于本文，文献[4]对抗样本经典方法进行了较为全面的介绍，细致分析了对抗样本的产生原因、特征以及评价指标，但其对近年来出现的新方法介绍略显不足。文献[5]极为详细的阐述了近来对抗样本的生成与防御方法，其更多地是提供方法概览，分析归纳对比的部分不足。本文主要贡献在于：以视觉对抗样本生成技术为代表，根据感知度量指标的不同定义，挂一漏万地将对抗样本生成模型从信号层、内容层和语义层进行概括性的分类、总结和归纳，并就对抗样本产生的原因、机理进行了深入分析。

# 机器学习中的安全问题 {#problem}

## 问题描述 {#statement}

## 对抗样本生成 {#attack}

# 对抗样本生成技术介绍 {#survey}

## 白盒攻击 {#white-box}

## 黑盒攻击 {#black-box}

## 物理空间对抗样本 {#physical}

# 生物识别系统的对抗样本 {#face}

jkjk

# 总结与展望 {#conclusion}

jkjk

`r if (knitr::is_html_output()) '## 参考文献 {-}'`
